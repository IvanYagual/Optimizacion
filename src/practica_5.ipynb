{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 5: Automatic Differentation (AD) in Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic Differentation (AD) está implementada en varias librerias de Python: por ejemplo, en TensorFlow aparece en el módulo [tf.autodiff](https://www.tensorflow.org/api_docs/python/tf/autodiff) y en Pytorch en el módulo [TORCH.AUTOGRAD](https://pytorch.org/docs/stable/autograd.html). Por supuesto, AD también está presente (aunque de manera subyacente) en los modelos del módulo [scikit-learn](https://scikit-learn.org/stable/) dedicado a Machine Learning.\n",
    "\n",
    "\n",
    "[Autograd](https://autograd.readthedocs.io/en/latest/usage.html) es otra pequeña librería, basada en Numpy, que resulta muy conveniente para entender las bases de AD.\n",
    "\n",
    "En esta práctica nos centraremos en el estudio de los métodos implementados en TensorFlow y Pytorch (que actualmente son las librerías de Machine Learning más utilizadas) para calcular gradientes via AD. Veremos una primera introducción a backward AD en TensorFlow y dejaremos para el ejercicio de esta práctica su uso en Pytorch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  AD en TensorFlow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si no tenemos TensorFlow en nuestro entorno virtual, entonces hemos de incorporarlo al kernel de Python. Esto lo podemos hacer desde el Powershell Prompt de Anaconda escribiendo:\n",
    "\n",
    "**conda create -n tf tensorflow**\n",
    "\n",
    "**conda activate tf**\n",
    "\n",
    "A continuación cambiamos al kernel de tf y ya podemos importar TensorFlow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[tf.autodiff](https://www.tensorflow.org/api_docs/python/tf/autodiff) es el módulo de TensorFlow para AD. Se compone de dos clases:\n",
    "\n",
    "1) [tf.autodiff.ForwardAccumulator](https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator) para el modo forward, y\n",
    "\n",
    "2) [tf.GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape) para el modo backward.\n",
    "\n",
    "Nos centramos a continuación en la clase **backward** con [tf.GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape) .\n",
    "\n",
    "Veamos un primer ejemplo elemental: derivada de la función $y = x^2$ en $x=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "derivada de y respecto a x en x=3 igual a 6.0\n"
     ]
    }
   ],
   "source": [
    " \n",
    "x = tf.Variable(3.0) # asigna el valor 3 a la variable x\n",
    "\n",
    "with tf.GradientTape() as tape: # almacena todas las operaciones hechas con tf.GradientTape en tape\n",
    "  y = x ** 2\n",
    "\n",
    "dy_dx = tape.gradient(y, x) # derivada de y respecto a x\n",
    "print(f\"derivada de y respecto a x en x=3 igual a {dy_dx}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcular derivadas de orden superior procedemos así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "derivada de y respecto a x en x=3 igual a 27.0\n",
      "derivada segunda de y respecto a x dos veces en x=3 igual a 18.0\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape2: # almacena todas las operaciones hechas con tf.GradientTape en tape\n",
    "  with tf.GradientTape() as tape1:\n",
    "    y = x ** 3\n",
    "  dy_dx = tape1.gradient(y,x)  # derivada de y respecto a x\n",
    "d2y_d2x = tape2.gradient(dy_dx, x) # derivada segunda de y respecto a x dos veces\n",
    "\n",
    "print(f\"derivada de y respecto a x en x=3 igual a {dy_dx}\")\n",
    "print(f\"derivada segunda de y respecto a x dos veces en x=3 igual a {d2y_d2x}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideremos ahora una función de dos variables\n",
    "\n",
    "$$\n",
    "f(x_1, x_2) = x_1^2(x_1 + x_2)\n",
    "$$\n",
    "\n",
    "Vamos a calcular su gradiente en $x_1 = 2$, $x_2 = 3$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([2., 3.], dtype=float32)>\n",
      "f(2, 3) = 20.0\n",
      "gradiente de f en (2, 3) = [24.  4.]\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([2.0, 3.0]) \n",
    "\n",
    "print(x)\n",
    "\n",
    "with tf.GradientTape() as tape: # almacena todas las operaciones hechas con tf.GradientTape en tape\n",
    "  f = x[0]** 2 * (x[0] + x[1])\n",
    "\n",
    "print(f\"f(2, 3) = {f}\")\n",
    "\n",
    "grad_f = tape.gradient(f, x) # gradiente de f\n",
    "print(f\"gradiente de f en (2, 3) = {grad_f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz Hessiana y sus valores propios se pueden calcular así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matriz hessiana de f en (2, 3) = \n",
      " [[18.  4.]\n",
      " [ 4.  0.]]\n",
      "valores propios de la matriz hessiana de f en (2, 3) = \n",
      " [18.848858  -0.8488578]\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape2: # almacenamos todas las operaciones hechas con tf.GradientTape en tape2\n",
    "    with tf.GradientTape() as tape1:\n",
    "        f = x[0] ** 2 * (x[0] + x[1])\n",
    "    grad_f = tape1.gradient(f, x) # gradiente de f\n",
    "hess_f = tape2.jacobian(grad_f, x) # hessiana de f\n",
    "print(f\"matriz hessiana de f en (2, 3) = \\n {hess_f}\")\n",
    "\n",
    "from numpy.linalg import eig\n",
    "\n",
    "eigenvalues, _ = eig(hess_f)\n",
    "\n",
    "print(f\"valores propios de la matriz hessiana de f en (2, 3) = \\n {eigenvalues}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora un modelo más elaborado. En concreto, vamos a calcular el gradiente de \n",
    "$$\n",
    "y = x * w  + b\n",
    "$$\n",
    "donde $x$ es un vector fila de $4$ componentes, $w$ es una matriz $4\\times 3$ y $b$ un vector columna de $3$ componentes. Obsérvese que $y$ es una función de $\\mathbb{R}^4 \\to \\mathbb{R}^3$, ya que \n",
    "$$\n",
    "y = x * w  + b = (x_1, x_2, x_3, x_4) \n",
    "\\left(\n",
    "    \\begin{array}{ccc}\n",
    "    w_{11} & w_{12} & w_{13} \\\\\n",
    "    w_{21} & w_{22} & w_{23} \\\\\n",
    "    w_{31} & w_{32} & w_{33} \\\\\n",
    "    w_{41} & w_{42} & w_{43} \n",
    "    \\end{array}\n",
    "\\right) + \n",
    "\\begin{pmatrix}\n",
    "b_1 \\\\ b_2 \\\\ b_3\n",
    "\\end{pmatrix}\n",
    "= \n",
    "\\begin{pmatrix}\n",
    "x_1 w_{11} + x_2 w_{21} + x_3 w_{31} + x_4 w_{41} +b_1\\\\\n",
    "x_1 w_{12} + x_2 w_{22} + x_3 w_{32} + x_4 w_{42} +b_2\\\\\n",
    "x_1 w_{13} + x_2 w_{23} + x_3 w_{33} + x_4 w_{43} +b_3\\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empezamos borrando tape\n",
    "del tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'x:0' shape=(1, 4) dtype=float32, numpy=array([[1., 2., 3., 4.]], dtype=float32)>\n",
      "<tf.Variable 'w:0' shape=(4, 3) dtype=float32, numpy=\n",
      "array([[ 14.927208 ,  16.68613  ,  18.564335 ],\n",
      "       [  3.6685467,   4.7894325,  -6.357937 ],\n",
      "       [-15.565605 ,  -4.2436075, -13.076515 ],\n",
      "       [  2.9025555, -13.434191 ,  -4.547262 ]], dtype=float32)>\n",
      "<tf.Variable 'b:0' shape=(3,) dtype=float32, numpy=array([1., 1., 1.], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# A continuación, definimos las variables asignando valores aleatorios a los pesos\n",
    "# y unos a los biases\n",
    "tensor_x = tf.Variable([[1, 2, 3, 4]], dtype=tf.float32, name='x')\n",
    "tensor_w = tf.Variable(tf.random.uniform((4, 3), minval=-20, maxval=20, \n",
    "                       dtype=tf.float32), name='w') # w: weight\n",
    "tensor_b = tf.Variable(tf.ones(3, dtype=tf.float32), name='b') # bias\n",
    "print(tensor_x)\n",
    "print(tensor_w)\n",
    "print(tensor_b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además de la función $y = x * w + b$ consideramos la función de pérdida\n",
    "\n",
    "$$\n",
    "\\text{loss } = \\frac{1}{3} \\sum_{j=1}^3 (y_j - (y_{\\text{label}})_j)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_y:  tf.Tensor([[-145.99689  -72.59936  -95.67115]], shape=(1, 3), dtype=float32)\n",
      "tensor_loss:  tf.Tensor(12303.048, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# definimos las funciones de las que queremos calcular sus gradientes\n",
    "# usaremos el atributo persistent=True para poder llamar varias veces a \n",
    "#  GradientTape.gradient(). De lo contrario, sólo lo podemos llamar una vez\n",
    "y_label = tf.constant([[1, 2, 3]], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:   \n",
    "  tensor_y = tensor_x @ tensor_w + tensor_b # \n",
    "  tensor_loss = tf.reduce_mean((tensor_y - y_label) **2 ) # dloss = mean(2*y) dy; dloss = (2*y[0] + 2*y[1] + 2*y[2])/3\n",
    "\n",
    "print('tensor_y: ', tensor_y)# Usar torch.no_grad para evitar que se calcule el gradiente en los parámetros de la pérdida\n",
    "\n",
    "print('tensor_loss: ', tensor_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_dy_dx:  tf.Tensor([[ -6.441292  -32.46366    -6.8784275 -56.315872 ]], shape=(1, 4), dtype=float32)\n",
      "jac_dy_dx:  tf.Tensor(\n",
      "[[[[-14.195752   -4.2817736 -18.25697   -17.366667 ]]\n",
      "\n",
      "  [[ 19.2263    -16.567844    6.0485363 -19.458895 ]]\n",
      "\n",
      "  [[-11.471839  -11.614046    5.3300056 -19.490309 ]]]], shape=(1, 3, 1, 4), dtype=float32)\n",
      "tensor_dloss_dy:  tf.Tensor([[-97.997925 -49.732906 -65.78077 ]], shape=(1, 3), dtype=float32)\n",
      "tensor_dloss_dx:  tf.Tensor([[1189.601  2007.553  1137.7219 3951.7324]], shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# calculamos ahora varios gradientes\n",
    "\n",
    "tensor_dy_dx = tape.gradient(tensor_y, tensor_x) # gradiente de y respecto a x (derivada de y_i respecto x_j y sumar en i). Es decir, es un vector cuyas\n",
    "                                                #componenes son las sumas de las columnas de la matriz \n",
    "                                                # jacobia previamente calculada, esto es, su primera componente es la suma de todas\n",
    "                                                # las derivadas parciales respecto de x[0], y así sucesivamente\n",
    "tensor_jac_dy_dx = tape.jacobian(tensor_y, tensor_x) # matriz jacobiana de y respecto a x\n",
    "tensor_dloss_dy = tape.gradient(tensor_loss, tensor_y) # gradiente de loss respecto a y\n",
    "tensor_dloss_dx = tape.gradient(tensor_loss, tensor_x) # gradiente de loss respecto a x\n",
    "\n",
    "print('tensor_dy_dx: ', tensor_dy_dx)\n",
    "print('jac_dy_dx: ', tensor_jac_dy_dx)\n",
    "print('tensor_dloss_dy: ', tensor_dloss_dy)\n",
    "print('tensor_dloss_dx: ', tensor_dloss_dx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo dejamos aquí. Esto es sólo una pequeña introducción.\n",
    "\n",
    "Para saber más: [AT con TensorFlow](https://insights.willogy.io/tensorflow-part-3-automatic-differentiation/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optim-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
