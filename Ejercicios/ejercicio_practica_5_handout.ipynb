{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Ejercicio de la Práctica 5: Automatic Differentation (AD) in Pytorch"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","En esta ejercicio nos centraremos en la implementación de AD en Pytorch. En concreto, en el submódulo [TORCH.AUTOGRAD](https://pytorch.org/docs/stable/autograd.html) \n","\n","Si no tienes instalado Pytorch, lo primero que has de hacer es instalarlo. Una forma sencilla de hacerlo es en el **Powershell Prompt** de **Anaconda**, escribiendo:\n","\n","**conda install pytorch torchvision torchaudio pytorch-cuda=11.6 -c pytorch -c nvidia**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##  AD en Pytorch"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Carga Pytorch escribiendo \n","\n","**import torch**"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Completar aquí\n","import torch\n","import numpy as np\n","from torch.autograd.functional import hessian\n","from numpy.linalg import eig\n","from torch.autograd.functional import hessian\n","\n","# --------------------\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["El objetivo de este ejercicio es hacer los mismos cálculos que hicimos en la práctica de AD en TensorFlow pero ahora en Pytorch\n","\n","Veamos un primer ejemplo elemental: derivada de la función $y = x^2$ en $x=3$."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["derivada de y respecto a x = tensor([6.])\n"]}],"source":["# Completar aquí\n","x = torch.tensor([3.0], requires_grad=True)\n","\n","y = x ** 2\n","\n","# Calcula la derivada de y respecto a x\n","y.backward()\n","\n","# El gradiente de y respecto a x está en x.grad\n","dy_dx = x.grad\n","print(f\"derivada de y respecto a x = {dy_dx}\")\n","\n","# --------------------\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Consideremos ahora una función de dos variables\n","\n","$$\n","f(x_1, x_2) = x_1^2(x_1 + x_2)\n","$$\n","\n","Vamos a calcular su gradiente en $x_1 = 2$, $x_2 = 3$. "]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["gradiente de f respecto a x = tensor([24.,  4.])\n"]}],"source":["# Completar aquí\n","# tensor que requiere gradientes\n","x = torch.tensor([2.0, 3.0], requires_grad=True)\n","\n","f = x[0]**2 * (x[0] + x[1])\n","\n","# Calculamos el gradiente de f con respecto a x\n","f.backward()  # Esto calcula el gradiente\n","\n","# Accede al gradiente de x\n","grad_f = x.grad\n","print(f\"gradiente de f respecto a x = {grad_f}\")\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Calculamos ahora su matriz Hessiana en el mismo punto y los valores propios de dicha matriz"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Hessian matrix:\n"," tensor([[18.,  4.],\n","        [ 4.,  0.]])\n","valores propios de la matriz hessiana de f en (2, 3) = \n"," [18.848858  -0.8488578]\n"]}],"source":["def f(x):\n","    return x[0]**2 * (x[0] + x[1])\n","\n","x = torch.tensor([2.0, 3.0], requires_grad=True)\n","\n","hess_f = hessian(f, x)\n","print(\"Hessian matrix:\\n\", hess_f)\n","\n","\n","eigenvalues, _ = eig(hess_f)\n","\n","print(f\"valores propios de la matriz hessiana de f en (2, 3) = \\n {eigenvalues}\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Veamos ahora un modelo más elaborado. Consideremos la función vectorial \n","$$\n","y = x * w  + b\n","$$\n","donde $x$ es un vector fila de $4$ componentes, $w$ es una matriz $4\\times 3$ y $b$ un vector columna de $3$ componentes.\n","\n","Define la función anterior en **pytorch** y asigna los siguientes valores a la variables:\n","\n","1)  $x = [[1., 2., 3., 4.]]$\n","2)  $w$ valores aleatorios\n","3)  $b$ unos"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["x =  tensor([[1., 2., 3., 4.]], requires_grad=True)\n","w =\n"," tensor([[0.4792, 0.8692, 0.3657],\n","        [0.2091, 0.2768, 0.3109],\n","        [0.7517, 0.5404, 0.0791],\n","        [0.6101, 0.5677, 0.1531]])\n","b = tensor tensor([1., 1., 1.])\n","tensor_y: tensor([[6.5929, 6.3148, 2.8371]], grad_fn=<AddBackward0>)\n"]}],"source":["# Completar aquí\n","tensor_x = torch.tensor([[1, 2, 3, 4]], dtype=torch.float32, requires_grad=True)\n","tensor_w = torch.rand((4, 3), dtype=torch.float32)  # w: peso\n","tensor_b = torch.ones(3, dtype=torch.float32)  # bias\n","\n","tensor_y = tensor_x @ tensor_w + tensor_b\n","\n","print(\"x = \", tensor_x)\n","print(\"w =\\n\", tensor_w)\n","print(\"b = tensor\", tensor_b)\n","print(\"tensor_y:\", tensor_y)\n","\n","# --------------------\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Además de la función $y = x * w + b$ consideramos la función de pérdida\n","\n","$$\n","\\text{loss } = \\frac{1}{3} \\sum_{j=1}^3 (y_j - (y_{\\text{label}})_j)^2\n","$$\n","donde $y_{\\text{label}} = [[1., 2., 3.]]$\n","\n","Define la función loss en pytorch"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["loss:  tensor(16.6414)\n"]}],"source":["# Completar aqu\n","y_label = torch.tensor([[1, 2, 3]], dtype=torch.float32)\n","\n","# torch.no_grad evita que se calcule el gradiente en los parámetros de la pérdida\n","with torch.no_grad():\n","    # Calcula la salida\n","    tensor_y = tensor_x @ tensor_w + tensor_b  # tensor_y = x @ w + b\n","    # Calcula la pérdida\n","    tensor_loss = torch.mean((tensor_y - y_label) ** 2)  # Pérdida MSE\n","\n","print('loss: ', tensor_loss)\n","\n","# --------------------\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Calcula en pytorch los gradientes de la función loss, primero respecto a $y$ y después respecto a $x$"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["gradiente de loss respecto a y:  tensor([[ 3.7286,  2.8765, -0.1086]])\n","gradiente de loss respecto a x:  tensor([[4.2473, 1.5420, 4.3485, 3.8913]])\n"]},{"data":{"text/plain":["'\\nRecordar:\\nEs necesario usar el parámetro \"retain_graph=True\" porque PyTorch trabaja con gráficos computacionales.\\nCuando se calcula un gradiente con \"torch.autograd\", el gráfico se libera automáticamente para ahorrar memoria, \\nlo que impide calcular gradientes múltiples en el mismo gráfico y daría error.\\nEl parámetro en \"True\", le indica que mantenga el gráfico en memoria, permitiendo \\nrealizar varios calculos con el mismo gráfico computacional. \\n'"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Completar aquí\n","tensor_y = torch.matmul(tensor_x, tensor_w) + tensor_b  # tensor_y = x @ w + b\n","tensor_loss = torch.mean((tensor_y - y_label) ** 2)  # Pérdida\n","\n","# Gradiente de la pérdida respecto a y\n","tensor_dloss_dy = torch.autograd.grad(tensor_loss, tensor_y, retain_graph=True)[0]\n","\n","# Gradiente de la pérdida respecto a x\n","tensor_dloss_dx = torch.autograd.grad(tensor_loss, tensor_x)[0]\n","\n","print('gradiente de loss respecto a y: ', tensor_dloss_dy)\n","print('gradiente de loss respecto a x: ', tensor_dloss_dx)\n","# --------------------\n","\n","\n","'''\n","Recordar:\n","Es necesario usar el parámetro \"retain_graph=True\" porque PyTorch trabaja con gráficos computacionales.\n","Cuando se calcula un gradiente con \"torch.autograd\", el gráfico se libera automáticamente para ahorrar memoria, \n","lo que impide calcular gradientes múltiples en el mismo gráfico y daría error.\n","El parámetro en \"True\", le indica que mantenga el gráfico en memoria, permitiendo \n","realizar varios calculos con el mismo gráfico computacional. \n","'''\n"]}],"metadata":{"kernelspec":{"display_name":"optim-2","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
